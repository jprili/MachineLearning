{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc8c50c-b49f-4ee7-8fe4-271da7a18e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a510c9c7-8f7f-45c4-8080-88f7e39f5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_numspercent(num, arr):\n",
    "    print(np.count_nonzero(arr == num)/len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f7c6ba-912a-489f-b9ce-b5e9ea853117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.098\n",
      "0.1135\n",
      "0.1032\n",
      "0.101\n",
      "0.0982\n",
      "0.0892\n",
      "0.0958\n",
      "0.1028\n",
      "0.0974\n",
      "0.1009\n"
     ]
    }
   ],
   "source": [
    "mndata = MNIST(\"dat\")\n",
    "train = np.array(mndata.load_training()).T\n",
    "test = np.array(mndata.load_testing()).T\n",
    "\n",
    "testLabels = test[:, 1]\n",
    "\n",
    "for i in range(10):\n",
    "    count_numspercent(i, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a070e63-f1af-4c99-b0e8-6e0872a00cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printShape(name, arr):\n",
    "    print(\"{0} shape:\".format(name) + str(arr.shape))\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        constructor for a neural network.\n",
    "        from https://github.com/mnielsen/neural-networks-and-deep-learning/\n",
    "             blob/master/src/network.py\n",
    "        \"\"\"\n",
    "        self.nLayers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # array of bias vectors, list with `nLayers` amount of vecs\n",
    "        # containing `size` elements\n",
    "        \n",
    "        self.biases = [np.random.uniform(-0.5, 0.5, (nextSize, 1)) \\\n",
    "                       for nextSize in sizes[1:]]\n",
    "        \n",
    "        # array of weight matrices, if the previous layer is \n",
    "        # sized a, and next layer b, the weight matrix would\n",
    "        # be sized (b * a) to accomodate transformation\n",
    "        # (b * 1) = (b * a) . (a * 1)\n",
    "        \n",
    "        self.weights = [np.random.uniform(-0.5, 0.5, (nextSize, prevSize)) \\\n",
    "                        for prevSize, nextSize in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedFwd(self, arr):\n",
    "        \"\"\"\n",
    "        forward feeding\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            arr = self.reLU(np.dot(w, arr) + b)\n",
    "        \n",
    "        return arr\n",
    "    \n",
    "    def stochasticGradDesc(self, trainDat, nEpoch, sSSize, rate, testDat = None):\n",
    "        \n",
    "        print(\"starting ...\")\n",
    "        \n",
    "        if testDat is not None: \n",
    "            nTest = len(testDat)\n",
    "        \n",
    "        nTrain = len(trainDat)\n",
    "        \n",
    "        for i in range(nEpoch):\n",
    "            random.shuffle(trainDat)\n",
    "            \n",
    "            subSets = [train[k : k + sSSize] for k in range(0, nTrain, sSSize)]\n",
    "            \n",
    "            for subSet in subSets:\n",
    "                self.updSubSet(subSet, rate)\n",
    "                \n",
    "            if testDat is not None:\n",
    "                percent = (self.evaluate(testDat)/nTest)*100\n",
    "                print(\"Epoch {0} accuracy: {1}% = 100 * {2}/{3}.\".format( \\\n",
    "                    i, percent, self.evaluate(testDat), nTest))\n",
    "                \n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(i))\n",
    "                \n",
    "        print(\"end.\")\n",
    "        \n",
    "    def updSubSet(self, subSet, rate):\n",
    "        \n",
    "        # initialise weight gradients and bias gradients\n",
    "        \n",
    "        nabB = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabW = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        set_size = len(subSet)\n",
    "        \n",
    "        # update\n",
    "        \n",
    "        for x, y in subSet:\n",
    "            delNabB, delNabW = self.propBwd(x, y)\n",
    "            \n",
    "            nabB = [nB + dnB for nB, dnB in zip(nabB, delNabB)]\n",
    "            nabW = [nW + dnW for nW, dnW in zip(nabW, delNabW)]\n",
    "            \n",
    "            \n",
    "        self.weights = [Wi - ((rate / set_size) * nabWi) \\\n",
    "                        for Wi, nabWi in zip(self.weights, nabW)]\n",
    "        \n",
    "        self.biases = [Bi - ((rate / set_size) * nabBi) \\\n",
    "                       for Bi, nabBi in zip(self.biases, nabB)]\n",
    "        \n",
    "        \n",
    "    def evaluate(self, testDat):\n",
    "        results = [(np.argmax(self.feedFwd(x)), y) for (x, y) in testDat]\n",
    "        # tuple (int, int)\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "        \n",
    "        \n",
    "    def difCost(self, outAct, y):\n",
    "        return outAct - y # (10 * 1)\n",
    "        \n",
    "    def propBwd(self, x, y):\n",
    "        \"\"\"\n",
    "        backwards propagation\n",
    "        \n",
    "        x - activation layer: ndarray, size m * 1\n",
    "        y - output: int, could be converted to ndarray, size 10 * 1\n",
    "            with oneHot(y).\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialise weight gradients and bias gradients\n",
    "        \n",
    "        nabB = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabW = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # forward propagation\n",
    "        \n",
    "        act = np.array([x]).T\n",
    "        \n",
    "        acts = [act]\n",
    "        \n",
    "        zVecs = []\n",
    "        \n",
    "        # feedFwd is not called to store the z and a values\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, act) + b # (m * 1)\n",
    "            \n",
    "            zVecs.append(z)\n",
    "            \n",
    "            act = self.reLU(z)\n",
    "            acts.append(act)\n",
    "            \n",
    "        # bwd        \n",
    "        delta = self.difCost(acts[-1], self.oneHot(y))\n",
    "        \n",
    "        nabW[-1] = np.dot(delta, np.array(acts[-2]).T)\n",
    "        nabB[-1] = delta # (out * 1)\n",
    "        \n",
    "        # for the rest of the network\n",
    "        \n",
    "        for l in range(2, self.nLayers):\n",
    "            z = zVecs[-l]\n",
    "            \n",
    "            delta = (self.weights[-l + 1].T @ delta) * self.difReLU(z)\n",
    "            \n",
    "            nabW[-l] = delta @ acts[-l - 1].T # (m * 1). (1 * m) = (m * m)\n",
    "            nabB[-l] = delta # m * 1\n",
    "            \n",
    "        return nabB, nabW\n",
    "    \n",
    "    \n",
    "    ####### activation functions #######\n",
    "            \n",
    "    def smd(self, z):\n",
    "        \"\"\"sigmoid function\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def difsmd(self, z):\n",
    "        return self.smd(z) * (1 - self.smd(z))\n",
    "    \n",
    "    def oneHot(self, y):\n",
    "        arr = np.zeros(10)\n",
    "        arr[y] = 1\n",
    "        return np.array([arr]).T\n",
    "    \n",
    "    def reLU(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def difReLU(self, z):\n",
    "        if isinstance(z, np.ndarray):\n",
    "            z = np.array([i > 0 for i in z])\n",
    "            return z\n",
    "        \n",
    "        return z > 0\n",
    "    \n",
    "    def softMaxOne(self, z, z_arr):\n",
    "        return np.exp(z)/sum(np.exp(z_arr))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae724ed-018d-4a86-ac5b-0f0d025b1e56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting ...\n",
      "Epoch 0 accuracy: 9.8% = 100 * 980/10000.\n",
      "Epoch 1 accuracy: 9.8% = 100 * 980/10000.\n",
      "Epoch 2 accuracy: 9.8% = 100 * 980/10000.\n",
      "Epoch 3 accuracy: 9.8% = 100 * 980/10000.\n",
      "Epoch 4 accuracy: 9.8% = 100 * 980/10000.\n",
      "Epoch 5 accuracy: 9.8% = 100 * 980/10000.\n",
      "Epoch 6 accuracy: 9.8% = 100 * 980/10000.\n",
      "Epoch 7 accuracy: 9.8% = 100 * 980/10000.\n",
      "Epoch 8 accuracy: 9.8% = 100 * 980/10000.\n",
      "Epoch 9 accuracy: 9.8% = 100 * 980/10000.\n",
      "Epoch 10 accuracy: 9.8% = 100 * 980/10000.\n"
     ]
    }
   ],
   "source": [
    "neuNet = NeuralNetwork([784, 16, 10, 10])\n",
    "\n",
    "neuNet.stochasticGradDesc(train, 11, 10, 0.1, testDat = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7325c0f7-c723-4db2-94dc-d04398a69751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "[[ 0  0  0]\n",
      " [ 3  4  5]\n",
      " [12 14 16]]\n"
     ]
    }
   ],
   "source": [
    "arr_1 = np.arange(9).reshape(3, 3)\n",
    "arr_2 = np.array([np.arange(3)]).T\n",
    "print(arr_2.shape)\n",
    "print(arr_1 * arr_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
